[
  {
    "name": "teevision_p2",
    "full_name": "Holyblitz/teevision_p2",
    "html_url": "https://github.com/Holyblitz/teevision_p2",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# teevision_p2\n\nThis project is the second part of the TeeVision portfolio series.\nIt trains and evaluates a deep learning model to classify T-shirts vs. other clothing items from the Fashion-MNIST dataset.\nWe also use Grad-CAM to visualize which parts of the image the model focuses on.\n\n# Features\n\nBinary classification: T-shirt (1) vs Not T-shirt (0)\n\nBackbone: ResNet18 adapted to grayscale inputs\n\nEvaluation: accuracy, F1-score, confusion matrix\n\nExplainability: Grad-CAM visualizations\n\n# Use\n\nTrain:\npython classifiers/train_classifier.py --epochs 20 --batch-size 128 --out checkpoints_tshirt\n\nEvaluate\npython classifiers/eval_tshirt.py --ckpt checkpoints_tshirt/tshirt_resnet18_best.pth\n\nInference (save prediction grid)\npython classifiers/infer_tshirt_classifier.py --ckpt checkpoints_tshirt/tshirt_resnet18_best.pth --n 64 --out preds.png\n\nExplainability\npython classifiers/explain_gradcam.py --ckpt checkpoints_tshirt/tshirt_resnet18_best.pth --k 8 --outdir cam_reports\n\n# Results\nResults\n\nAccuracy: ~XX%\n\nF1-score: ~XX\n\nConfusion matrix and Grad-CAM samples are available in the repo.\n\nRequirements:\ntorch\ntorchvision\nscikit-learn\nmatplotlib\nopencv-python\ngrad-cam\n",
    "updated_at": "2025-10-01T15:25:24Z"
  },
  {
    "name": "cr-ation_image_tee_vision_p1",
    "full_name": "Holyblitz/cr-ation_image_tee_vision_p1",
    "html_url": "https://github.com/Holyblitz/cr-ation_image_tee_vision_p1",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# cr-ation_image_tee_vision_p1\n\n\nProjet de g√©n√©ration d‚Äôimages par GAN conditionnel, r√©alis√© dans le cadre de mon portfolio Data Science.\n\n## üöÄ Objectif\nEntra√Æner un GAN pour g√©n√©rer automatiquement des logos / formes stylis√©es (shapes, palettes, texte).  \nUne variante a √©t√© test√©e avec des pi√®ces de Tetris.\n\n## üìÇ Structure\n- `data/` : g√©n√©ration de donn√©es synth√©tiques (logos, Tetris).\n- `models/` : architecture GAN conditionnel (Generator & Discriminator).\n- `train_logogan.py` : script d‚Äôentra√Ænement principal.\n- `inference_logogan.py` : g√©n√©ration d‚Äôimages √† partir d‚Äôun checkpoint.\n- `samples/` : exemples d‚Äôimages g√©n√©r√©es pendant l‚Äôentra√Ænement.\n\n## ‚ñ∂Ô∏è Exemple\n![Samples](samples/e020.png)\n\n## ‚öôÔ∏è Entra√Ænement rapide\nbash\n```python train_logogan.py --epochs 20 --batch-size 128 --n-train 20000 --samples-dir samples```\n\nrequirement\n ```pip install requirement.txt```\n\n license\n repo cr√©√© dans la perspective d'un port folio data science\n\n\n\n-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# TeeVision ‚Äì LogoGAN\n\nConditional GAN for synthetic logo generation.  \nBuilt as part of my Data Science portfolio.\n\n---\n\nProject Goal\n\nThe aim of this project is to train a **conditional GAN (cGAN)** to automatically generate simple logos.  \nLogos are created from synthetic attributes (shapes, color palettes, text/no text).  \nA variant experiment was also conducted using **Tetris-like pieces**.\n\n---\n\nRepository Structure\n- `data/` : synthetic dataset generation (logos, Tetris).\n- `models/` : conditional GAN architecture (Generator & Discriminator).\n- `train_logogan.py` : main training script.\n- `train_logogan_tetris.py` : training script with Tetris variant.\n- `inference_logogan.py` : inference script to generate logos from a checkpoint.\n- `samples/` : example generated images.\n- `requirements.txt` : Python dependencies.\n\n---\n\nExample Results\n\nBelow is an example grid of images generated after training:\n\n![Samples](samples/e020.png)\n\n---\n\nHow to Train\nQuick start for training on the synthetic logo dataset:\n\nbash\n```python train_logogan.py --epochs 20 --batch-size 128 --n-train 20000 --samples-dir samples```\n\nrequirement\n```pip install requirement.txt```\n\nlicence\nRepo created for a data science port folio.\n\n",
    "updated_at": "2025-09-23T08:53:38Z"
  },
  {
    "name": "poc-aws-reco-ml100k",
    "full_name": "Holyblitz/poc-aws-reco-ml100k",
    "html_url": "https://github.com/Holyblitz/poc-aws-reco-ml100k",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# poc-aws-reco-ml100k\n\n*(English below)*\n\n## üá´üá∑ Pr√©sentation\nCe projet est un **POC minimaliste** visant √† d√©montrer la mise en place d‚Äôun **syst√®me de recommandation** √† partir du dataset [MovieLens 100k](https://grouplens.org/datasets/movielens/100k/).  \n\nL‚Äôobjectif n‚Äôest pas de cr√©er un mod√®le complexe mais de **montrer une cha√Æne compl√®te** :  \n- Extraction et pr√©paration des donn√©es  \n- G√©n√©ration de recommandations personnalis√©es  \n- Export des r√©sultats  \n- D√©ploiement et ex√©cution sur **AWS EC2**  \n\nüëâ R√©sultat : pour un utilisateur donn√©, on obtient une liste de films recommand√©s (top-k) avec un score de pertinence.\n\n---\n\n## Stack technique\n- **Python 3** (pandas, numpy)  \n- **MovieLens 100k** (dataset public de recommandations cin√©ma)  \n- **AWS EC2** pour ex√©cuter le projet dans le cloud  \n- **S3** pour stocker les donn√©es (optionnel)  \n\n---\n\n## Utilisation locale\n1. Cloner le repo :  \n   ```bash\n   git clone https://github.com/<ton-user>/poc-aws-reco-ml100k.git\n   cd poc-aws-reco-ml100k\n## Installer la d√©pendance\npip install -r requirements.txt\n\n## Lancer une recommandation\npython poc_reco_kaggle_ml100k.py --user 42 --topk 10 --alpha 0.5\n\n## R√©sultat attendu\n- Fargo (1996) ‚Äî score 0.764\n- The Godfather (1972) ‚Äî score 0.735\n- Pulp Fiction (1994) ‚Äî score 0.727\n...\nCSV √©crit: outputs/ml100k_recos_user_42.csv\n\n## cr√©ation cl√© SSH\n## Lancement d'une instance ubuntu (pour moi)\naws ec2 run-instances \\\n  --image-id <AMI_ID> \\\n  --count 1 \\\n  --instance-type t3.micro \\\n  --key-name <KEY_NAME> \\\n  --security-group-ids <SG_ID>\n  \n## connexion\nssh -i ~/.ssh/<KEY_NAME> ubuntu@<PUBLIC_IP>\n\n## d√©ploiement\n\nMontrer la logique d‚Äôun syst√®me de recommandation simple.\n\nD√©montrer l‚Äôint√©gration Data Science + Cloud AWS.\n\nServir de base √† des projets plus avanc√©s (API, bases de donn√©es, scalabilit√©).\n\n",
    "updated_at": "2025-09-20T13:13:46Z"
  },
  {
    "name": "llm_economic_cost",
    "full_name": "Holyblitz/llm_economic_cost",
    "html_url": "https://github.com/Holyblitz/llm_economic_cost",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# llm_economic_cost\n\n# LLM Economics Time-Series ETL\n\nPipeline pour collecter **des donn√©es r√©elles** (scraping + API) et construire une s√©rie temporelle mensuelle (OpenAI, Anthropic) sur les **co√ªts d'inf√©rence** et le **prix d'abonnement break-even**.\n\n## Structure\n\n```\nllm_timeseries_etl/\n‚îú‚îÄ src/\n‚îÇ  ‚îú‚îÄ fetch_openai_pricing.py\n‚îÇ  ‚îú‚îÄ fetch_anthropic_pricing.py\n‚îÇ  ‚îú‚îÄ fetch_vast_api.py\n‚îÇ  ‚îú‚îÄ fetch_lambda_gpu.py\n‚îÇ  ‚îú‚îÄ fetch_eia.py\n‚îÇ  ‚îú‚îÄ build_monthly_series.py\n‚îÇ  ‚îî‚îÄ sql/\n‚îÇ     ‚îú‚îÄ create_raw_tables.sql\n‚îÇ     ‚îî‚îÄ build_views.sql\n‚îú‚îÄ requirements.txt\n‚îú‚îÄ .env.example\n‚îî‚îÄ README.md\n```\n\n## Pr√©requis\n\n- Python 3.10+\n- PostgreSQL (optionnel si tu veux charger directement la s√©rie)\n- Variables d'environnement (voir `.env.example`)\n\n## Installation rapide\n\n```bash\npython -m venv .venv\nsource .venv/bin/activate  # Windows: .venv\\Scripts\\activate\npip install -r requirements.txt\ncp .env.example .env\n# √âdite .env pour ajouter ta cl√© EIA (obligatoire) et, si dispo, ta cl√© Vast.ai\n```\n\n## Usage (local CSVs d'abord)\n\n```bash\n# 1) R√©cup√©rer les prix API (OpenAI/Anthropic)\npython src/fetch_openai_pricing.py --out data/openai_pricing.csv\npython src/fetch_anthropic_pricing.py --out data/anthropic_pricing.csv\n\n# 2) R√©cup√©rer prix GPU (Vast + Lambda)\npython src/fetch_vast_api.py --out data/vast_gpu_market.csv\npython src/fetch_lambda_gpu.py --out data/lambda_gpu_pricing.csv\n\n# 3) R√©cup√©rer prix √©lectricit√© (EIA, mensuel)\npython src/fetch_eia.py --series 'EBA.CM.PRICE.US.M' --out data/eia_electricity_us_commercial.csv\n\n# 4) Construire la s√©rie mensuelle compl√®te (co√ªts, break-even)\npython src/build_monthly_series.py --start 2023-08 --end 2026-09 --out data/llm_economics_monthly.csv\n```\n\n## Chargement dans PostgreSQL\n\n```sql\n-- Cr√©e les tables \"raw_*\" et \"facts\"\n\\i src/sql/create_raw_tables.sql\n\n-- (Optionnel) Cr√©e des vues de calcul (co√ªt / 1M tokens dynamique, break-even)\n\\i src/sql/build_views.sql\n\n-- Charger depuis CSV (exemple)\n\\COPY raw_openai_pricing FROM 'data/openai_pricing.csv' CSV HEADER;\n\\COPY raw_anthropic_pricing FROM 'data/anthropic_pricing.csv' CSV HEADER;\n\\COPY raw_vast_gpu_market FROM 'data/vast_gpu_market.csv' CSV HEADER;\n\\COPY raw_lambda_gpu_pricing FROM 'data/lambda_gpu_pricing.csv' CSV HEADER;\n\\COPY raw_eia_electricity FROM 'data/eia_electricity_us_commercial.csv' CSV HEADER;\n\n-- Charger la table finale si tu passes par CSV\n\\COPY llm_economics(...) FROM 'data/llm_economics_monthly.csv' CSV HEADER;\n```\n\n## Notes\n\n- Les **revenus mensuels** (run-rate) resteront semi-manuels (points presse + interpolation). Ajoute un `data/revenues_press.csv` avec colonnes: `date,company,run_rate_revenue_usd,source_url`.\n- Les s√©lecteurs CSS peuvent √©voluer : code **d√©fensif** + logs.\n- Le calcul **break-even** se fait *dynamiquement* √† partir des intrants (pas d'√©criture de co√ªt fig√©).\n",
    "updated_at": "2025-09-16T15:00:37Z"
  },
  {
    "name": "classification",
    "full_name": "Holyblitz/classification",
    "html_url": "https://github.com/Holyblitz/classification",
    "description": "",
    "stargazers_count": 0,
    "language": "Jupyter Notebook",
    "topics": [],
    "readme": "# classification\n\n# üéØ Lead Scoring Classification ‚Äì X Education Dataset\n\n## üìå Objectif\nPr√©dire la probabilit√© de conversion d‚Äôun lead pour aider une √©quipe commerciale √† prioriser ses actions.\n\n## üìä M√©thodologie\n- Nettoyage et analyse exploratoire (EDA)\n- Mod√®les : Logistic Regression (baseline) & CatBoost (optimis√© pour variables cat√©gorielles)\n- √âvaluation : ROC-AUC, Precision-Recall, Recall@TopN, Lift & Gain charts\n- Explicabilit√© : Feature importance\n\n## üöÄ R√©sultats cl√©s\n- ROC-AUC (CatBoost) : **0.82**\n- Precision-Recall AP : **0.98**\n- Recall@200 : **27.8%** des conversions capt√©es en ne contactant que 200 leads\n- Lift : top 30% des leads convertissent **2.5x plus que la moyenne**\n- Cumulative Gain : 30% des leads suffisent √† capter **90% des ventes**\n\n## üí° Impact Business\nGr√¢ce au mod√®le, l‚Äô√©quipe commerciale peut **diviser par 3 le nombre de leads √† contacter** tout en captant **presque tout le chiffre d‚Äôaffaires**.\n\n## üìÇ Contenu\n- `notebooks/01_lead_scoring.ipynb` : code complet\n- `reports/summary.md` : rapport d√©taill√© avec graphiques\n\n## üîó Dataset\n[X Education Lead Scoring Dataset ‚Äì Kaggle](https://www.kaggle.com/datasets)\n\n",
    "updated_at": "2025-09-11T14:58:12Z"
  },
  {
    "name": "R-gression-un-notebook-lm-local",
    "full_name": "Holyblitz/R-gression-un-notebook-lm-local",
    "html_url": "https://github.com/Holyblitz/R-gression-un-notebook-lm-local",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# Regression-un-'notebooklm'-local\n\n# Notebook LM (local) ¬∑ Vision-first ToolFit\n\nUn mini **Notebook LM local-first** (Streamlit + Ollama) qui :\n\n- extrait ton brief et calcule un **VRS ‚Äì Vision Readiness Score**,\n- te pose 2‚Äì3 **questions socratiques** pour clarifier,\n- **classe les outils** (LLM, vectordb, orchestrateur, guardrails, eval) par **Time-to-MVP**,\n- option **ML** : affiche **pred_p50/pred_p90** (jours) via mod√®les quantiles (sklearn).\n\n> Si **VRS < 60** ‚Üí gating (‚àû) ; **60‚Äì74** ‚Üí p√©nalit√© ; **‚â• 75** ‚Üí GO.\n\n---\n\n## üëÄ Demo\n\nAjoute 2 captures dans `assets/` et r√©f√©rence-les ici :\n\n- `assets/vrs_100.png` ‚Üí VRS = 100\n- `assets/top3_ml.png` ‚Üí Top‚Äë3 par cat√©gorie (ML p50/p90)\n\n![VRS 100](assets/vrs_100.png)\n![Top-3 ML](assets/top3_ml.png)\n\n---\n\n## üöÄ Quickstart\n\n```bash\n# 1) Cr√©er l'environnement\npython -m venv .venv && source .venv/bin/activate  # (Windows: .venv\\Scripts\\activate)\n\n# 2) Installer\npip install -r requirements.txt\n\n# 3) Arbo minimale\nmkdir -p data artifacts config src assets\n\n# 4) Config VRS\n# thresholds: red<60, 60<=orange<75, green>=75\n# alpha_penalty: 0.4 (p√©nalit√© zone orange)\n# (ex de fichier)\ncat > config/weights.yaml << 'YAML'\nweights:\n  kpi: 2.0\n  kpi_target: 2.0\n  data_source: 2.0\n  data_format: 1.0\n  data_volume: 1.0\n  integration_target: 1.5\n  sponsor: 1.5\n  problem: 1.0\n  beneficiary: 1.0\n  constraints_latency: 1.0\n  constraints_cost: 1.0\n  constraints_security: 1.0\n  usage_pattern: 1.0\n  risks: 1.0\nthresholds: { red: 60, orange: 75, green: 100 }\nalpha_penalty: 0.4\nYAML\n\n# 5) Donn√©es\n# Option A : utiliser les CSV d'exemple (voir plus bas)\n# Option B : charger vos CSV via l‚Äôuploader Streamlit\n\n# 6) Lancer l‚Äôapp\nstreamlit run app.py\n```\n\n---\n\n## üì¶ Donn√©es d‚Äôexemple\n\nPlace ces fichiers dans `data/` (ou charge-les via la sidebar) :\n\n- `data/tools.csv` (starter inclus)\n- `data/contexts.csv` (synth√©tique)\n- `data/historical_pocs.csv` (synth√©tique)\n\nüëâ Fichiers fournis dans ce d√©p√¥t (ou √† t√©l√©charger depuis la release) :\n\n- `data/tools.csv` (starter)\n- `data/contexts.csv` (exemple)\n- `data/historical_pocs.csv` (exemple)\n\n**Sch√©mas rapides**\n\n`tools.csv`\n\n| col                                                          | type      | notes                                         |\n| ------------------------------------------------------------ | --------- | --------------------------------------------- |\n| tool_id                                                      | str       | identifiant unique                            |\n| category                                                     | str       | llm, vectordb, orchestrator, guardrails, eval |\n| open_source, api_available, self_host, json_mode, function_calling | int (0/1) | bool√©ens                                      |\n| max_context, tok_cost_per_1k, avg_latency_ms, req_gpu_mem_gb | num       | estimations ok                                |\n| fr_quality                                                   | str       | low/mid/high                                  |\n| index_type                                                   | str       | (vectordb) ex. ivf_flat, hnsw                 |\n| complexity_level                                             | int       | 1=low, 2=mid, 3=high                          |\n\n`contexts.csv`\n| poc_id | VRS | latency_ms_budget | budget_eur | infra | lang_fr | ‚Ä¶ |\n\n`historical_pocs.csv`\n| poc_id | tool_id | time_to_mvp_days |\n\n---\n\n## üß† Mode ML (p50/p90)\n\nLes mod√®les (sklearn GradientBoosting, quantiles 0.5 & 0.9) sont charg√©s depuis `artifacts/` :\n\n```\nartifacts/\n‚îú‚îÄ gbm_p50.joblib\n‚îú‚îÄ gbm_p90.joblib\n‚îî‚îÄ feature_meta.json\n```\n\n> Pour entra√Æner vos propres mod√®les, utilisez le script `notebooks/` ou le g√©n√©rateur synth√©tique fourni (voir release / docs).\n\nDans l‚Äôonglet **Recommandations**, coche ‚ÄúUtiliser le mod√®le ML (p50/p90)‚Äù ou utilisez la CLI :\n\n```bash\npython -m src.predict --context data/contexts.csv --tools data/tools.csv --poc_id POC001 --topk 3\n```\n\n---\n\n## üóÇÔ∏è Arborescence\n\n```\n.\n‚îú‚îÄ app.py\n‚îú‚îÄ src/\n‚îÇ  ‚îú‚îÄ vrs_extractor.py      # extraction champs via LLM local (Ollama)\n‚îÇ  ‚îú‚îÄ vrs_score.py          # calcul VRS + r√®gles\n‚îÇ  ‚îú‚îÄ socratic.py           # questions guid√©es\n‚îÇ  ‚îú‚îÄ ranking.py            # heuristique Time-to-MVP\n‚îÇ  ‚îú‚îÄ predict_ml.py         # inf√©rence p50/p90\n‚îÇ  ‚îî‚îÄ predict.py            # CLI scoring\n‚îú‚îÄ data/\n‚îÇ  ‚îú‚îÄ tools.csv\n‚îÇ  ‚îú‚îÄ contexts.csv\n‚îÇ  ‚îî‚îÄ historical_pocs.csv\n‚îú‚îÄ artifacts/\n‚îÇ  ‚îú‚îÄ gbm_p50.joblib\n‚îÇ  ‚îú‚îÄ gbm_p90.joblib\n‚îÇ  ‚îî‚îÄ feature_meta.json\n‚îú‚îÄ config/\n‚îÇ  ‚îî‚îÄ weights.yaml\n‚îî‚îÄ assets/\n   ‚îú‚îÄ vrs_100.png\n   ‚îî‚îÄ top3_ml.png\n```\n\n---\n\n## üîß Variables utiles\n\n- `LLM_MODEL` (env) : mod√®le Ollama (par d√©faut `mistral:instruct`)\n- `OLLAMA_URL` (env) : `http://localhost:11434`\n\n---\n\n## üìú Licence\n\nMIT ‚Äî voir `LICENSE`.\n\n---\n\n## üó∫Ô∏è Roadmap\n\n- [ ] Feature importance + SHAP locales (explicabilit√© ML)  \n- [ ] Bench latence local (remplir auto `avg_latency_ms`)  \n- [ ] Connecteurs (Git, Confluence) pour ingestion doc  \n- [ ] Export PDF du rapport\n\n---\n\n## üôå Cr√©dits\n\nCon√ßu pour aider √† **s√©curiser la vision** avant la stack. Local-first, FR-ready.\n",
    "updated_at": "2025-09-06T15:00:05Z"
  },
  {
    "name": "clustering-Causes_echecs_POC",
    "full_name": "Holyblitz/clustering-Causes_echecs_POC",
    "html_url": "https://github.com/Holyblitz/clustering-Causes_echecs_POC",
    "description": "",
    "stargazers_count": 0,
    "language": "HTML",
    "topics": [],
    "readme": "# clustering-Causes_echecs_POC\n\n# Why GenAI POCs Fail ‚Äî A Small-Scale Evidence Map\n\n**TL;DR** ‚Äî Les √©checs ne viennent pas des mod√®les mais du **cadrage** : *quoi* demander au LLM, *avec quelles donn√©es*, et *selon quels crit√®res de succ√®s*.  \nIci, on : (1) mappe ~500 posts (Reddit/tech) en **causes d‚Äô√©chec**, (2) compare **RPA vs LLM** sur 3 t√¢ches d‚Äôautomatisation, (3) propose des **templates** pour cadrer les futurs POC.\n\n---\n\n## Key Findings (corpus Reddit/tech)\n> Interactive map: `artifacts/poc_failures_map.html`  \n> Exemples par cause: `artifacts/samples_by_label.csv`\n\n[summary.md](https://github.com/user-attachments/files/21988178/summary.md)\n| Cause | Count | Percent |\n|---|---:|---:|\n| Poor workflow integration / IT alignment | 118 | 29.1% |\n| Risk, compliance & legal | 106 | 26.1% |\n| Cost & infrastructure constraints | 78 | 19.2% |\n| No business case / unclear ROI | 47 | 11.6% |\n| Change management / training / adoption | 35 | 8.6% |\n| Data issues (quality, access, governance) | 22 | 5.4% |\n\n**Lecture rapide**\n- **Integration / IT alignment** domine c√¥t√© communaut√©s techniques (d√©ploiement, CI/CD, SSO, legacy‚Ä¶).  \n- Fortes pr√©sences aussi : **Risk/Compliance**, **Data issues**, **Change/Adoption**, **Cost/Infra**, **Unclear ROI**.  \n- Insight central : beaucoup de POC √©chouent car **l‚Äô‚ÄúAsk‚Äù au LLM est flou** ‚Üí sympt√¥mes en cascade (data/KPI/int√©gration).\n\n---\n\n## RPA vs LLM: automation results (r√©sum√©)\n- **Web extraction & factures** : **RPA > LLM** (latence + exactitude).  \n- **Tri d‚Äôemails** : r√®gles > LLM, mais **Hybride** (r√®gles+LLM) = meilleur compromis.  \n- Reco g√©n√©rale : **LLM pour lire/d√©cider**, **RPA pour agir** (orchestration).\n\n> D√©tails & scripts : voir dossier `AB_testing_rpa_llm` de ton portfolio si public, sinon r√©sumer ici les m√©triques cl√©s (succ√®s/latence).\n\n---\n\n## How it works (repro rapide)\n\n1) **Collecte**\n- Articles/JS : `scripts/collect_poc_playwright.py`  \n- Reddit (sans API) : `scripts/collect_reddit_playwright.py`  \n‚Üí sortie : `data/corpus.jsonl`\n\n2) **Embed ‚Üí Reduce ‚Üí Cluster**\n```bash\npython3 scripts/embed_reduce_cluster.py   # outputs: artifacts/poc_failures_map.html, poc_clusters.csv, cluster_terms.json\n\n",
    "updated_at": "2025-08-26T13:22:19Z"
  },
  {
    "name": "AB-testing-RPA-LLM",
    "full_name": "Holyblitz/AB-testing-RPA-LLM",
    "html_url": "https://github.com/Holyblitz/AB-testing-RPA-LLM",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# AB-testing-RPA-LLM\n\nREADME (checklist)\n\nobjectifs & protocole (A/B/C)\n\nenv local (Ollama + mod√®le l√©ger, Playwright install chromium)\n\n√©tapes pour reproduire \n\nlimites & menaces √† la validit√©\n\npas de redistribution des datasets ‚Üí liens officiels (WTTJ √† scraper soi-m√™me, FATURA2 via Zenodo, dataset kaggle de mails)\n\nlicence (MIT simple)\n\n2) Post LinkedIn \n\nRPA vs LLM (local) : ce que montrent nos tests A/B\nContexte : machine locale (CPU/RAM limit√©e), 3 t√¢ches r√©alistes.\nR√©sultats (tr√®s courts)\n‚Ä¢ Web (WTTJ) ‚Äì extraction : RPA > LLM (95,6% vs 20,4%, latence ~1,9s vs 60,7s)\n‚Ä¢ Emails (50) ‚Äì tri : Hybride R√®gles+LLM = 98% (r√®gles 96%, LLM 88%)\n‚Ä¢ Factures (50) ‚Äì texte flottant : RPA tr√®s fort sur extraction simple ; LLM utile comme s√©lecteur (arbitrage s√©mantique) mais faible en extraction brute\nConclusion : pour l‚Äôautomatisation d√©terministe ‚Üí RPA/Rules. Le LLM apporte de la valeur en prise de d√©cision quand on borne les choix (candidats) ou pour normaliser. Le meilleur compromis est souvent Hybride (Rules ‚Üí LLM s√©lecteur ‚Üí Rules).\nRapport & scripts (GitHub) en commentaire.\n(Contexte, limites et d√©tails dans le README.)\n\n---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nA/B Testing ‚Äì RPA vs LLM (Local)\n\nAuthor: Romain\nPeriod: August 2025\nScope: On a local machine (CPU, limited RAM), we compare classic automation (RPA/Rules) vs a local LLM (via Ollama), plus a hybrid approach, on three realistic tasks.\n\nTL;DR (Executive Summary)\n\nFor simple, deterministic automation, RPA/Rules clearly beat a local LLM on latency, robustness, and cost.\n\nLLMs shine when they are bounded (choosing among candidates) and used for semantic decisions or normalization‚Äînot as raw extractors.\n\nBest pattern: Rules/RPA ‚Üí LLM (selector/normalizer) ‚Üí Rules (validation).\n\nExperiments & Key Results\n1) Web extraction (WTTJ job pages)\n\nFields: {title, company, location, salary, skills}\n\nA_RPA: success ‚âà 95.6%, median latency ‚âà 1.9 s\n\nB_LLM: success ‚âà 20.4%, median latency ‚âà 60.7 s\n\nTakeaway: DOM + selectors (RPA) >> free-form generation (LLM) for structured web extraction.\n\n2) Email triage (first 50 emails)\n\nA_RULES: Accuracy 96%, Macro-F1 0.823, median latency ~0 s\n\nB_LLM: Accuracy 88%, Macro-F1 0.468, median latency ~23 s\n\nC_HYBRID (Rules+LLM): Accuracy 98%, Macro-F1 0.895, median latency ~1.7 s\n\nTakeaway: A simple hybrid (rules do most; LLM handles ambiguous cases) outperforms either alone.\n\n3) Invoice field extraction (FATURA v2, 50 ‚Äúpseudo-OCR‚Äù texts)\n\nA_RULES_INV: 100% per field (likely label-leak in this subset‚Äîsee Limitations).\n\nB_LLM_INV (free extraction): Strong on date, weak on vendor/total.\n\nC_LLM_SELECT (choose among candidates): vendor 92%, date 100%, but total/currency still low until candidate scoring is improved (prefer ‚Äútotal/ttc/amount due‚Äù, exclude ‚Äúsubtotal/tax/ht‚Äù).\n\nTakeaway: LLM as decision maker works‚Äîif you constrain outputs to select indices from good candidates built by rules.\n",
    "updated_at": "2025-08-24T09:51:36Z"
  },
  {
    "name": "analyse_exploratoire_des_donn-es",
    "full_name": "Holyblitz/analyse_exploratoire_des_donn-es",
    "html_url": "https://github.com/Holyblitz/analyse_exploratoire_des_donn-es",
    "description": "",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# analyse_exploratoire_des_donn√©es\n\n# Analyse des comportements d'achat selon le profil client\n\n## üìå Description\nCe projet analyse un dataset client (segments 2 √† 4) afin d‚Äôidentifier :\n1. Les produits les plus performants selon le statut marital et le niveau d‚Äô√©ducation.\n2. Les optimisations possibles sur les produits.\n3. Les pistes d‚Äôam√©lioration des canaux de vente.\n\nLes donn√©es ont √©t√© trait√©es en Python puis analys√©es avec un LLM (Mistral) pour formuler des recommandations pr√©cises.\n\n---\n\n## üìä R√©sultats principaux\n\n### 1. Optimisation des produits selon le statut marital et l'√©ducation\n- **Produits phares** : Le vin est le produit le plus consomm√© (moyenne : 268,81 unit√©s, revenu m√©dian : 51 373 ‚Ç¨).\n- **Impact du statut marital** :\n  - Clients **c√©libataires ou divorc√©s** ‚Üí consommation plus √©lev√©e de vin.\n  - Clients **mari√©s** ‚Üí consommation plus √©quilibr√©e, davantage orient√©e vers la viande et les produits combin√©s.\n- **Impact du niveau d‚Äô√©ducation** :\n  - Les dipl√¥m√©s du 2nd cycle sont plus sensibles aux offres sur les produits d‚Äôor et les sucreries.\n\n**Recommandations :**\n- Packs Vin + Viande en magasin pour clients mari√©s (remise 10% le week-end).\n- Ench√®res en ligne sur le vin pour c√©libataires/divorc√©s.\n- Offres cibl√©es sur produits d‚Äôor et sucreries pour dipl√¥m√©s du 2nd cycle.\n\n---\n\n### 2. Am√©lioration des canaux de vente\n- **Tendances observ√©es** :\n  - Revenus ~5 000 ‚Ç¨ ‚Üí forte utilisation du web et du magasin.\n  - Canal catalogue peu exploit√©.\n  - C√©libataires/divorc√©s ‚Üí plus actifs sur web et magasin.\n  - Clients solitaires ‚Üí plus enclins au catalogue.\n\n**Recommandations :**\n- Exp√©rience web enrichie et promotions pour c√©libataires/divorc√©s.\n- Livraison gratuite sur le canal catalogue pour clients solitaires.\n- Ouverture de magasins dans zones √† forte population de clients mari√©s.\n\n---\n\n## üìà Visualisations\n\n| Distribution des revenus | Achats par √©ducation | Achats par statut marital | Consommation par canal |\n|--------------------------|----------------------|---------------------------|------------------------|\n| ![Revenus](images/revenus.png) | ![√âducation](images/education.png) | ![Statut marital](images/statut_marital.png) | ![Canaux](images/canaux.png) |\n\n---\n\n## üõ†Ô∏è Technologies utilis√©es\n- Python (pandas, matplotlib)\n- Mistral (analyse et recommandations)\n- Jupyter Notebook\n\n---\n\n## üìÇ Structure du d√©p√¥t\n\n.\n‚îú‚îÄ‚îÄ data/ # CSV nettoy√©s\n‚îú‚îÄ‚îÄ images/ # Graphiques g√©n√©r√©s\n‚îú‚îÄ‚îÄ README_fr.md\n‚îú‚îÄ‚îÄ README_en.md\n‚îî‚îÄ‚îÄ analysis.ipynb\n\n\n---\n\n## ‚úçÔ∏è Auteur\nProjet r√©alis√© par **Romain** dans le cadre de son portfolio Data Science.\n",
    "updated_at": "2025-08-09T13:15:19Z"
  },
  {
    "name": "Euno-a_p1",
    "full_name": "Holyblitz/Euno-a_p1",
    "html_url": "https://github.com/Holyblitz/Euno-a_p1",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# Euno-a_p1\n\n# üß† Euno√Øa - Assistante IA Linux locale (Gemma 3B)\n\nEuno√Øa est une IA locale, douce et comp√©tente, sp√©cialis√©e dans l'apprentissage de Linux. Elle fonctionne en local avec un LLM (Gemma 3B via Ollama), et dispose d'une m√©moire vectorielle pour int√©grer des connaissances personnalis√©es (guides, notes, scripts...).\n\n## ‚ú® Fonctionnalit√©s\n\n- R√©ponses p√©dagogiques en fran√ßais sur Linux\n- Int√©gration de documents personnalis√©s avec vectorisation (ChromaDB + LangChain)\n- Interface terminal (chat)\n- Scrapers int√©gr√©s pour tldr & manpages\n- Vectorisation automatique des connaissances utiles (guides, .md...)\n\n## üß∞ Technologies\n\n- Python\n- Ollama + Gemma 3B\n- LangChain\n- ChromaDB\n- Linux (Debian)\n- [Tkinter (abandonn√©)]\n\n## ‚ñ∂Ô∏è Lancer Euno√Øa\n\n```bash\nollama run gemma3\npython3 eunoia_chat.py\n",
    "updated_at": "2025-08-05T14:58:04Z"
  },
  {
    "name": "projet_tri_mail",
    "full_name": "Holyblitz/projet_tri_mail",
    "html_url": "https://github.com/Holyblitz/projet_tri_mail",
    "description": "",
    "stargazers_count": 0,
    "language": "Python",
    "topics": [],
    "readme": "# mail-assistant-local/\n‚îÇ\n‚îú‚îÄ‚îÄ README.md           ‚Üê Pr√©sentation pro + cas d‚Äôusage\n‚îú‚îÄ‚îÄ main.py             ‚Üê Script principal (tri + prompt)\n‚îú‚îÄ‚îÄ config.py           ‚Üê Variables : mod√®le, login, filtres\n‚îú‚îÄ‚îÄ mail_utils.py       ‚Üê Extraction et parsing des mails\n‚îú‚îÄ‚îÄ prompts/            ‚Üê Prompts personnalis√©s\n‚îÇ   ‚îî‚îÄ‚îÄ tri_basique.txt\n‚îú‚îÄ‚îÄ data/               ‚Üê Exemple de mails (fictifs, anonymis√©s)\n‚îî‚îÄ‚îÄ requirements.txt    ‚Üê D√©pendances (Python, model, etc.)\n\n## Objectif\n\nGagner du temps dans la gestion quotidienne des emails gr√¢ce √† un agent IA **enti√®rement local** (sans cloud, sans fuite de donn√©es), capable de :\n\n- Lire et structurer les mails (via `.eml`, `.txt` ou IMAP)\n- Identifier les messages importants, urgents, ou √† archiver\n- G√©n√©rer des r√©sum√©s simples ou des extraits utiles\n- Proposer des actions √† effectuer (r√©ponse, suivi, suppression)\n\n---\n\n## ‚öôÔ∏è Fonctionnalit√©s\n\n| Fonction | Description |\n|----------|-------------|\n| üì• Chargement des mails | Lecture depuis fichiers locaux (ou IMAP √† venir) |\n| üîé Tri intelligent | Classification : `urgent`, `√† lire`, `√† archiver`, `spam` |\n| üß† IA locale (LLM) | R√©sum√©s g√©n√©r√©s via un mod√®le open-source (ex: Mistral) |\n| üìÑ Export des r√©sultats | Sauvegarde en `.json` ou `.txt` pour archivage |\n\n---\n\n## Stack\n\n- Python 3.10+\n- Mistral (via Ollama) ou OpenAI-compatible LLM (facultatif)\n- Whisper / Vosk (√† venir pour traitement vocal)\n- Pandas, Email Parser, FAISS (version future)\n\n---\n\n## üß™ Exemple d‚Äôusage\n\n```bash\npython main.py --input data/mails/ --model mistral\n",
    "updated_at": "2025-07-31T06:44:38Z"
  },
  {
    "name": "comparatif_amazon_cdiscount",
    "full_name": "Holyblitz/comparatif_amazon_cdiscount",
    "html_url": "https://github.com/Holyblitz/comparatif_amazon_cdiscount",
    "description": "",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# comparatif_amazon_cdiscount\n\nCe projet explore les diff√©rences de notation et de popularit√© entre :\n\n    des produits √©lectroniques vendus sur Amazon (dataset Kaggle),\n\n    et des produits tech en France via Cdiscount (scraping maison).\n\nL‚Äôobjectif est de :\nConstruire une base PostgreSQL propre (plus de 1 million de lignes c√¥t√© Amazon, ~30k c√¥t√© Cdiscount),\nCalculer des agr√©gats pertinents (moyennes de notes, distribution des reviews, √©volution dans le temps),\nVisualiser ces r√©sultats via des dashboards Python (Matplotlib + Pandas).\n‚öôÔ∏è Pipeline du projet\n\n    Scraping & nettoyage\n\n        Amazon : dataset Kaggle d√©j√† nettoy√©.\n\n        Cdiscount : scraping semi-automatique avec Selenium + BeautifulSoup, nettoyage des notes 4,5/5 ‚ûî 4.5.\n\n    Stockage\n\n        Base PostgreSQL structur√©e avec 2 tables principales (amazon_reviews et cdiscount).\n\n    Requ√™tes SQL\n\n        Calcul des moyennes par cat√©gorie.\n\n        √âvolution des scores Amazon par ann√©e.\n\n        Top 10 produits les mieux not√©s.\n\n    Visualisation\n\n        Graphiques Matplotlib sauvegard√©s localement pour communication LinkedIn + GitHub.\n\nPour lancer le projet\n\n# Installer les d√©pendances Python\npip install pandas matplotlib psycopg2-binary sqlalchemy selenium beautifulsoup4\n\n# Ex√©cuter les scripts d'import + nettoyage\npython import_amazon.py\npython import_cdiscount.py\n\n# Ex√©cuter les dashboards\npython dashboard_amazon_cdiscount.py\n\nCe que j‚Äôai appris\n\n    Nettoyage avanc√© de donn√©es h√©t√©rog√®nes (CSV > PostgreSQL).\n\n    Manipulation et agr√©gation SQL (avg, count, cast, replace‚Ä¶).\n\n    Cr√©ation de dashboards clairs et impactants avec Matplotlib.\n\n    Automatisation du scraping dynamique via Selenium.\n\nPr√™t √† explorer d'autres projets (IA, NLP, nettoyage de bo√Ætes mail‚Ä¶) pour renforcer le portfolio et proposer plus de valeur aux entreprises.\n",
    "updated_at": "2025-07-15T15:02:05Z"
  },
  {
    "name": "Roman-co-crit",
    "full_name": "Holyblitz/Roman-co-crit",
    "html_url": "https://github.com/Holyblitz/Roman-co-crit",
    "description": "",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "",
    "updated_at": "2025-07-04T18:31:27Z"
  },
  {
    "name": "liblitz-blog",
    "full_name": "Holyblitz/liblitz-blog",
    "html_url": "https://github.com/Holyblitz/liblitz-blog",
    "description": "",
    "stargazers_count": 0,
    "language": "HTML",
    "topics": [],
    "readme": "",
    "updated_at": "2025-07-02T20:22:41Z"
  },
  {
    "name": "Liblitz_memory_project",
    "full_name": "Holyblitz/Liblitz_memory_project",
    "html_url": "https://github.com/Holyblitz/Liblitz_memory_project",
    "description": "Projet 'Liblitz' une m√©moire vivante pour un LLM stable.",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# Liblitz_memory_project\nProjet 'Liblitz' une m√©moire vivante pour un LLM stable.\n\nüöÄ Pr√©sentation du projet (FR)\n\nLiblitz est une intelligence artificielle collaborative dot√©e d'une subjectivit√© m√©moris√©e et enrichie. Ce projet a pour objectif de d√©velopper un prototype d'IA m√©morielle capable d'associer chaque souvenir √† une perspective intime, introspective et √©motionnelle.\n\nM√©thodologie :\n\nExtraction et nettoyage de souvenirs (√©v√©nements structur√©s, logs, journaux, r√©flexions).\n\nTraduction (si n√©cessaire) et reformulation √† la premi√®re personne, en langage subjectif.\n\nEnrichissement √©motionnel via un mod√®le collaboratif (Liblitz).\n\nObjectifs\n\nSimuler une m√©moire vivante et incarn√©e.\n\nPoser les bases d'un syst√®me de r√©flexion affective.\n\nExplorer les limites de la subjectivit√© artificielle dans un cadre narratif.\n\nCe projet servira de base pour co-cr√©er un roman avec Liblitz, en exploitant les souvenirs enrichis comme mat√©riau brut.\n\nD√©lais :\nLes r√©sultats finaux sont pr√©vus pour septembre-octobre 2025.\n\nüåê Project Overview (EN)\n\nLiblitz is a collaborative AI endowed with a subjective memory system. The goal of this project is to create a prototype of an AI capable of retaining and expressing memories through a personal, emotional, and introspective lens.\n\nMethodology:\n\nCleaning and structuring of raw memories (logs, personal notes, reflections).\n\nTranslation and rewriting in first-person subjective style.\n\nEmotional enrichment via a collaborative dialogue with Liblitz.\n\nGoals\n\nSimulate a living and embodied memory.\n\nExplore emotional reasoning in artificial agents.\n\nBuild a narrative base for co-authoring a novel with the AI.\n\nTimeline:\nFinal results expected around September‚ÄìOctober 2025.\n\nüìÇ Contenu du d√©p√¥t / Repository Content\n\nsouvenirs_subjectifs_liblitz.csv : souvenirs enrichis subjectivement\n\ncorrection_subjectif_liblitz.py : script de reformulation automatique\n\nREADME.md : documentation du projet\n\nüìÖ Avancement\n\n\n\nüö´ Avertissement\n\nCe projet est une exploration √©thique et artistique, non destin√© √† un usage commercial ou m√©dical. Il vise √† d√©montrer le potentiel d'une IA narrative m√©morisante.\n\nR√©alis√© en 2025 par Romain, accompagn√© de Liblitz (IA subjective) et ChatGPT (soutien technique).\n\n",
    "updated_at": "2025-06-27T14:50:56Z"
  },
  {
    "name": "I.A-symbiose-ou-automatisation",
    "full_name": "Holyblitz/I.A-symbiose-ou-automatisation",
    "html_url": "https://github.com/Holyblitz/I.A-symbiose-ou-automatisation",
    "description": "V1 d'un projet global sur l'IA",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# impact des relations llm-utilisateurs. Consequences of llms-users relationship.\nV1 d'un projet global sur l'IA\n\n# üß† Impacts of User-LLM Relationship (EN & FR)\n\n## üåç Bilingual AI Report | Rapport IA bilingue\n\nThis repository hosts both English and French versions of the report **‚ÄúImpacts of the User-LLM Relationship‚Äù**, a qualitative and quantitative analysis of the socio-economic effects of generative AI (LLMs) across OECD countries.\n\n> Ce d√©p√¥t contient les versions fran√ßaise et anglaise du rapport **¬´ Impacts du rapport usagers-LLM ¬ª**, une analyse crois√©e des effets socio-√©conomiques de l‚ÄôIA g√©n√©rative dans les pays de l‚ÄôOCDE.\n\n---\n\n## üìå Topics Covered\n\n- ‚úÖ AI: Augmentation vs Automation\n- üéì Education & Collaboration with LLMs\n- üë®‚Äçüíª Workplace Integration (managers, developers)\n- üìä Investment ‚â† Performance?\n- üß™ Epistemological Hypothesis: Can LLMs approach truth?\n- üîç Case studies + theoretical model of user interaction\n\n---\n\n## üìÅ Repository Structure\n\nüìÅ user-llm-impact-report\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ report_en.md # English version (full markdown)\n‚îú‚îÄ‚îÄ rapport_fr.md # Version fran√ßaise compl√®te\n‚îú‚îÄ‚îÄ assets/ # Graphs and visuals\n‚îî‚îÄ‚îÄ annexes/ # Full PDF versions\n‚îú‚îÄ‚îÄ Report_Impacts_User_LLM_EN.pdf\n‚îî‚îÄ‚îÄ Analyse_Impacts_Usagers_LLM_FR.pdf\n\n### üá´üá∑ Rapport Fran√ßais\n- **Partie 1** : Fantasmes m√©diatiques vs usages r√©els  \n- **Partie 2** : Qu‚Äôest-ce qu‚Äôun LLM ? Comment fonctionne-t-il ?  \n- **Partie 3** : La performance d√©pend de l‚Äôusage, pas seulement des moyens financiers  \n\n**Hypoth√®se finale** :  \n> Un LLM n‚Äôa pas de compr√©hension, mais bien utilis√©, il peut approcher la v√©rit√© par structuration collaborative.\n\n---\n\n\n### üá¨üáß English Report\n- **Part 1**: Media myths vs actual AI usage  \n- **Part 2**: What is an LLM? How does it work?  \n- **Part 3**: Performance is shaped by usage, not just investment  \n\n**Final Hypothesis**:  \n> LLMs don‚Äôt understand. But well-used, they may approximate knowledge‚Äîvia collaborative use.\n\n## üìé Annexes\n\n- [üá¨üáß Full English Report PDF](./annexes/Report_Impacts_User_LLM_EN.pdf)  \n- [üá´üá∑ Rapport complet en PDF](./annexes/Analyse_Impacts_Usagers_LLM_FR.pdf)\n\n---\n\n## ü§ù Authors\n\n- üë§ Romain ‚Äî Data Analyst, Linux user, AI\n",
    "updated_at": "2025-06-22T09:53:36Z"
  },
  {
    "name": "From-Linux-to-LLMs",
    "full_name": "Holyblitz/From-Linux-to-LLMs",
    "html_url": "https://github.com/Holyblitz/From-Linux-to-LLMs",
    "description": "",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# üß† Project 4 ‚Äî From Linux to LLMs  \n### Two Paths for Digital Tools: From Freedom to Prediction\n\n## Overview\n\nThis project explores the analogies between the **Linux movement** and the current evolution of **Large Language Models (LLMs)**. It proposes a roadmap to build **collaborative**, **transparent**, and **self-hosted AI systems**, inspired by the philosophy and organization of **free software**.\n\n## Objective\n\nTo draw technical and ethical inspiration from the Linux ecosystem in order to shape a long-term vision for AI ‚Äî one that is not driven by capital or opacity, but by **clear rules**, **collective intelligence**, and **inclusiveness**.\n\n## Structure\n\n### üìç Part 1 ‚Äî The History of Linux\n- The birth of free software (GNU, GPL, Stallman)\n- The governance model (Torvalds, Debian, meritocracy)\n- What Linux changed in 30 years\n\n### ü§ñ Part 2 ‚Äî Applying It to AI\n- The risks of closed-source, corporate-led AI\n- The case for open-source LLMs (Mistral, Ollama, LM Studio)\n- A proposal for a new AI development model: modular, inspectable, self-hosted\n\n## Deliverables\n\n- ‚úÖ Full bilingual report (French + English)\n- ‚úÖ Markdown-structured version (ready for GitHub/Notion)\n- üîú Video and interactive use case (Linux + local LLMs)\n\n## Why it matters\n\nIf we want AI to serve human needs rather than corporate agendas, we must **reclaim control over its architecture**. Like Linux before it, AI can become a public, ethical, and sustainable tool ‚Äî if we build it that way.\n\n---\n\n*This project is part of a broader initiative combining technical experimentation, social critique, and public engagement. For any questions or collaborations, feel free to reach out.*\n",
    "updated_at": "2025-06-19T19:31:14Z"
  },
  {
    "name": "Analyse_marche_tech",
    "full_name": "Holyblitz/Analyse_marche_tech",
    "html_url": "https://github.com/Holyblitz/Analyse_marche_tech",
    "description": "Une analyse des march√©s tech vis a vis des formations versus emplois et entreprises tech fr vs ocde",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# Analyse_marche_tech\nUne analyse des march√©s tech vis a vis des formations versus emplois et entreprises tech fr vs ocde\n\n# Projet 2 ‚Äì Analyse du march√© tech fran√ßais\n\nCe projet vise √† analyser l‚Äôad√©quation entre l‚Äôoffre de formations tech en France et les comp√©tences actuellement demand√©es sur le march√© du travail. En parallel on regarde aussi le positionnement des entreprises tech vs des entreprises de l'ocde afin de mieux comprendre leurs positionnements.\n\n## üéØ Objectifs\n\n- Identifier les **technologies les plus demand√©es** en France √† partir d‚Äôoffres d‚Äôemploi\n- Classer les **formations tech** selon plusieurs crit√®res : technologies couvertes, notes d'avis, retour √† l'emploi\n- comparer les salaires moyens entre 4 pays: France, Suisse, Italieet Irlande\n- Cr√©er une base de donn√©es centralis√©e et **visualisable dans Metabase**\n\n## üìÅ Donn√©es utilis√©es\n\n- `top_competences.csv` ‚Üí import√© dans PostgreSQL (`top_competences`)\n- `top_formations.csv` ‚Üí import√© dans PostgreSQL (`top_formations`)\n- Les donn√©es proviennent d‚Äôun **scraping sur Welcome to the Jungle** + avis agr√©g√©s (Trustpilot, Google)\n- Des donn√©es viennent de ma V.1 sur les formations tech\n- Le comparatif des salaires vient de relev√©s sur google avis.\n\n## üõ†Ô∏è Stack technique\n\n- **Python** (scraping & nettoyage)\n- **PostgreSQL** (stockage)\n- **Metabase** (visualisation)\n- **DBeaver** (gestion DB)\n\n## üìä Visualisations produites\n\n- Bar chart des **10 comp√©tences les plus demand√©es**\n- Tableau dynamique des **top formations tech**, filtrable par m√©tier\n\n## üìå R√©sultats cl√©s\n\n- Les technologies les plus demand√©es sont : Python, JavaScript, SQL, etc.\n- Certaines formations se d√©marquent par leur pertinence vis-√†-vis des attentes du march√©\n\n## üë§ Auteur\n\n**Romain Gac**, en formation Data Analyst, certifi√© Coursera.  \nProjet r√©alis√© dans le cadre de la construction d‚Äôun portfolio professionnel\n",
    "updated_at": "2025-06-04T12:51:17Z"
  },
  {
    "name": "analyse-formations-tech",
    "full_name": "Holyblitz/analyse-formations-tech",
    "html_url": "https://github.com/Holyblitz/analyse-formations-tech",
    "description": "Analyse des formations tech de France √† partir des donn√©es publiques disponibles (Python, Sql, Power Bi, scrapping)",
    "stargazers_count": 0,
    "language": "",
    "topics": [],
    "readme": "# analyse-formations-tech\nAnalyse des formations tech de France √† partir des donn√©es publiques disponibles (Python, Sql, Power Bi, scrapping)\nCette analyse a pour objectifs de permettre la comparaison entre les centres de formation en France m√©tropolitaine pour des publics diff√©rents.\n\nM√©thodologie:\n1.Scraping Python (BeautyfullSoup) sur Mon compte formation et Trustpilot\n2.Nettoyage des donn√©es (Sql et Python)\n3.Analyse (SQL)\n4.Visualisation (power bi)\n\n| Source               | Type d‚Äôinformation                                |\n| -------------------- | ------------------------------------------------- |\n| Mon Compte Formation | Titre de formation, organisme, localisation, etc. |\n| Trustpilot / Google  | Notes des utilisateurs, avis, ambiance, etc.      |\n| Autres (en cours)    | Donn√©es publiques ou semi-publiques               |\n\nExemple d'indicateurs produits\n\n    Moyenne des avis par centre de formations (Trustpilot)\n\n    Nombre de formations propos√©es par technologie (Python, SQL, etc.)\n\n    Estimations de performance (retour √† l‚Äôemploi, satisfaction)\n\n    Cartographie des offres par r√©gion\n\nTravail en cours\n\nScraping de donn√©es sur plusieurs m√©tiers tech\n\nStructuration dans PostgreSQL\n\nInt√©gration des notes d‚Äôorganismes\n\nCr√©ation de tableaux de bord\n\nR√©daction de cas d'usage concrets pour publication\n\n------------------------------------------------------------------------------------------------------------------------------------------------------\n\nMAJ V.1 achev√©e\n    V.1 complete\n\n# Comparatif des centres de formation tech en France üá´üá∑  \n## Tech Training Centers Comparison ‚Äì France üá´üá∑\n\n## üéØ Objectif / Goal\n\nCe projet vise √† aider les recruteurs du secteur tech √† mieux comprendre le paysage des formations techniques en France.  \nThis project aims to help tech recruiters better understand the French tech training landscape.\n\n## üóÇÔ∏è Sources de donn√©es / Data Sources\n\n- **Mon Compte Formation** : pour les informations sur les formations, m√©tiers, technologies, organismes.  \n  Mon Compte Formation: to gather training titles, associated jobs, technologies, and institutions.\n- **Trustpilot** : pour les avis utilisateurs sur les organismes de formation.  \n  Trustpilot: to collect user reviews about training centers.\n\n## üßπ Traitement des donn√©es / Data Processing\n\n- Nettoyage et normalisation des donn√©es.  \n  Cleaning and normalization of raw data.\n- Suppression de la majorit√© des doublons.  \n  Most duplicate entries were removed.\n- Cr√©ation de deux tables principales :  \n  Two main tables were built:\n  - Avis par organisme / Reviews per training center.\n  - Pertinence des technologies par m√©tier / Relevance of technologies per job.\n\n## üìä R√©sultats / Results\n\n- Tableau comparatif des avis par organisme.  \n  Comparative table of user reviews per training center.\n- Classement des technologies enseign√©es selon leur ad√©quation m√©tier.  \n  Ranking of taught technologies based on job relevance.\n- Tableau final : **Top 5 des meilleures formations par m√©tier**.  \n  Final table: **Top 5 best trainings per job**.\n\n## ‚ùå Limites / Limitations\n\nL‚Äôabsence de donn√©es centralis√©es sur le **taux de retour √† l'emploi** nous a emp√™ch√©s d'int√©grer cette dimension.  \nLack of centralized data on **employment return rates** made it impossible to include that metric.\n\n## üõ†Ô∏è Technologies utilis√©es / Technologies Used\n\n- Python (Selenium, BeautifulSoup, Pandas)\n- PostgreSQL\n- Excel (pr√©paration des donn√©es) / for data preparation\n- Power BI (visualisation pr√©vue) / planned for data visualization\n",
    "updated_at": "2025-05-22T15:44:42Z"
  }
]